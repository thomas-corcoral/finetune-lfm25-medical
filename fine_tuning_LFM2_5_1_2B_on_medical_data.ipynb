{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tec7FtHFuaWT"
   },
   "source": [
    "# Fine-tuning LFM2.5-1.2B on Medical Data\n",
    "\n",
    "This notebook demonstrates how to fine-tune Liquid AI's LFM2.5-1.2B-Instruct model (Architecture: 16 layers (10 double-gated LIV convolution blocks + 6 GQA blocks)) on medical instruction data using Unsloth.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: T4 (free on Google Colab)\n",
    "- RAM: 12GB+\n",
    "- Time: ~15-20 minutes for 100 steps\n",
    "\n",
    "**Important:**\n",
    "1. Runtime â†’ Change runtime type â†’ T4 GPU\n",
    "2. Run cells in order\n",
    "3. This model is for educational purposes only - not medical advice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRJtfEykvhLz"
   },
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:33:30.729394Z",
     "iopub.status.busy": "2026-02-03T13:33:30.728879Z",
     "iopub.status.idle": "2026-02-03T13:34:15.713665Z",
     "shell.execute_reply": "2026-02-03T13:34:15.712611Z",
     "shell.execute_reply.started": "2026-02-03T13:33:30.729366Z"
    },
    "id": "FFmo3nM40_Uw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.57.3\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:34:15.716139Z",
     "iopub.status.busy": "2026-02-03T13:34:15.715641Z",
     "iopub.status.idle": "2026-02-03T13:34:15.719933Z",
     "shell.execute_reply": "2026-02-03T13:34:15.719213Z",
     "shell.execute_reply.started": "2026-02-03T13:34:15.716110Z"
    },
    "id": "VqFtlbRyYid-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --no-deps --force-reinstall unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3YieocAxGVz"
   },
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:34:15.721203Z",
     "iopub.status.busy": "2026-02-03T13:34:15.720882Z",
     "iopub.status.idle": "2026-02-03T13:35:21.184230Z",
     "shell.execute_reply": "2026-02-03T13:35:21.183365Z",
     "shell.execute_reply.started": "2026-02-03T13:34:15.721142Z"
    },
    "id": "XT7XdbZsvlav",
    "outputId": "70c4fdc5-336d-45b4-eb1e-515e087a53a5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 13:34:28.182320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770125668.583232      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770125668.694449      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770125669.732554      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770125669.732603      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770125669.732606      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770125669.732608      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n",
      "GPU Memory: 14.56 GB\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xViExoWJxKpO"
   },
   "source": [
    "## 3. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:35:21.186623Z",
     "iopub.status.busy": "2026-02-03T13:35:21.185922Z",
     "iopub.status.idle": "2026-02-03T13:35:43.347641Z",
     "shell.execute_reply": "2026-02-03T13:35:43.346831Z",
     "shell.execute_reply.started": "2026-02-03T13:35:21.186591Z"
    },
    "id": "Kmkk-cFBxNeM",
    "outputId": "e8cd62b7-bdd9-48ba-90b3-5d6ea138c7d7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.4: Fast Lfm2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.563 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b060ada484d09be81c0a14b158d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f63b7fa7d245c8883e9e5cbed3ff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaf48a2a7924d9bae347fc6d8008091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0815745ae1994ad590b567f16b6f1132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088c17407263434489f59cc7c77f81ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499cd35a20534c0898998a96e777c1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
    "\n",
    "# Load model with 16-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/LFM2.5-1.2B-Instruct\",\n",
    "    max_seq_length = 2048, # Can go up to 32,768 for LFM2.5\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
    "    load_in_16bit = True, # Enables 16bit LoRA\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    "    # device_map = \"balanced\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:35:43.349228Z",
     "iopub.status.busy": "2026-02-03T13:35:43.348819Z",
     "iopub.status.idle": "2026-02-03T13:36:04.473281Z",
     "shell.execute_reply": "2026-02-03T13:36:04.472589Z",
     "shell.execute_reply.started": "2026-02-03T13:35:43.349144Z"
    },
    "id": "e8fYa2YtZ4d2",
    "outputId": "cbe6d661-5407-400f-beb8-a4397a89b2be",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes is a chronic condition characterized by elevated blood sugar levels due to problems with insulin production or use. Common symptoms include:\n",
      "\n",
      "1. **Increased Thirst (Polydipsia)** â€“ The body loses more fluids than usual, leading to frequent urination.\n",
      "2. **Frequent Urination (Polyuria)** â€“ Excess glucose in the blood draws water into the urine, increasing urine output.\n",
      "3. **Unexplained Weight Loss** â€“ Despite eating more, some people lose weight due to the body breaking down fat and muscle for energy.\n",
      "4. **Extreme Fatigue** â€“ High blood sugar can\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgeable medical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the common symptoms of diabetes?\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Liquid settings!\n",
    "    temperature = 0.1, top_k = 50, top_p = 0.1, repetition_penalty = 1.05,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQFFRl1IxSGH"
   },
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:04.474837Z",
     "iopub.status.busy": "2026-02-03T13:36:04.474556Z",
     "iopub.status.idle": "2026-02-03T13:36:10.047815Z",
     "shell.execute_reply": "2026-02-03T13:36:10.046978Z",
     "shell.execute_reply.started": "2026-02-03T13:36:04.474800Z"
    },
    "id": "mBIY5mb0xUmN",
    "outputId": "1448a1b5-5be2-409f-d58d-0bb7249a324d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Trainable params: 11,108,352 (0.94%)\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"in_proj\",\n",
    "                      \"w1\", \"w2\", \"w3\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I8QrpuBxXLK"
   },
   "source": [
    "## 5. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:10.049573Z",
     "iopub.status.busy": "2026-02-03T13:36:10.048934Z",
     "iopub.status.idle": "2026-02-03T13:36:11.707461Z",
     "shell.execute_reply": "2026-02-03T13:36:11.706783Z",
     "shell.execute_reply.started": "2026-02-03T13:36:10.049542Z"
    },
    "id": "No26hEMIxZ3z",
    "outputId": "880fefee-2a1d-4b80-8107-d6d4f32da4a1",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2067cb4c2b5e496eb1cdc9e96f8eb466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09c6a4358484c469a0e2da9ad68a614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "medical_meadow_wikidoc_patient_info.json:   0%|          | 0.00/3.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cbe67160ac464c8aa258f2cd4fe9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5942 samples\n",
      "\n",
      "Sample data:\n",
      "{'input': 'What are the symptoms of Allergy?', 'output': 'Allergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.', 'instruction': 'Answer this question truthfully'}\n"
     ]
    }
   ],
   "source": [
    "# Load medical dataset\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_wikidoc_patient_information\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "24bc3c5fba064192b051e44ec7adab54",
      "eb63265f2443411db8ed3c54691438fe",
      "558a95946b0d4eabbff017c9e1dfa1af",
      "1186ce5453f540269cac7850205ab8a6",
      "95eeff9b4e6b4a62bdb8324954295b84",
      "02cad245e24b46559639a410590b880c",
      "e8bcdbbcbd10433482a2d2babe18f740",
      "5cfca4d2bc8149a99c7b08950fb7569d",
      "f814bc273c9541698fb7df084536cbdf",
      "5c615fdd5ac74daf9a64aac7e67a6c32",
      "243cba3754de4c60992d2d2f168db6e2"
     ]
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:11.708777Z",
     "iopub.status.busy": "2026-02-03T13:36:11.708438Z",
     "iopub.status.idle": "2026-02-03T13:36:12.022012Z",
     "shell.execute_reply": "2026-02-03T13:36:12.021223Z",
     "shell.execute_reply.started": "2026-02-03T13:36:11.708747Z"
    },
    "id": "_CryDDmlxbrG",
    "outputId": "db45e43e-1c25-4453-a74c-531944233274",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca57a326809944ba86a4011edf9de070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting dataset:   0%|          | 0/5942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset formatted!\n"
     ]
    }
   ],
   "source": [
    "# Format dataset for LFM2.5\n",
    "def format_instruction(example):\n",
    "    # system_message = \"You are a knowledgeable medical assistant providing accurate health information. Always recommend consulting healthcare professionals for medical advice.\"\n",
    "\n",
    "    conversation = [\n",
    "        # {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": example.get('input', '')},\n",
    "        {\"role\": \"assistant\", \"content\": example.get('output', '')}\n",
    "    ]\n",
    "\n",
    "    return {\"text\": conversation}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(\n",
    "    format_instruction,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Formatting dataset\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Dataset formatted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.023577Z",
     "iopub.status.busy": "2026-02-03T13:36:12.023312Z",
     "iopub.status.idle": "2026-02-03T13:36:12.028939Z",
     "shell.execute_reply": "2026-02-03T13:36:12.028409Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.023551Z"
    },
    "id": "e8lT9hVxabQI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset_fmt = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.032231Z",
     "iopub.status.busy": "2026-02-03T13:36:12.031911Z",
     "iopub.status.idle": "2026-02-03T13:36:12.051141Z",
     "shell.execute_reply": "2026-02-03T13:36:12.050260Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.032202Z"
    },
    "id": "IwKzCwX8ahQB",
    "outputId": "d262ca1f-cce7-463f-c960-eb1b4eb36976",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [{'content': 'What are the symptoms of Allergy?', 'role': 'user'},\n",
       "  {'content': 'Allergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fmt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "169df539dfe545be9bf91691250c44e3",
      "f274c5d04fa24f18aada8c422efd3ce1",
      "b725c2474b604b0ba52546da7e110c20",
      "ca27b03757474d95b7a1d56bc0b8e48c",
      "08675a3ec3434f1c9e181e36eae11797",
      "343e411b7c7049c4908ff79f782db068",
      "e56d53606d974072a5eea27a656207d4",
      "ab40c94ed9dc40ef82741ef3fdc2a5fa",
      "efa4d6afc73c4376ae0fbce7014affdd",
      "2a46589983bf412cb0266b088bb2105f",
      "89a7024e75a4478a8f0690b58a8c80b4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.052606Z",
     "iopub.status.busy": "2026-02-03T13:36:12.052278Z",
     "iopub.status.idle": "2026-02-03T13:36:12.486191Z",
     "shell.execute_reply": "2026-02-03T13:36:12.485441Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.052566Z"
    },
    "id": "31XuOeGnat0L",
    "outputId": "377904cd-1ba5-4152-9c23-323604951abd",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d039217178842369b5267d553577ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    texts = tokenizer.apply_chat_template(\n",
    "        examples[\"text\"],\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = False,\n",
    "    )\n",
    "    return { \"text\" : [x.removeprefix(tokenizer.bos_token) for x in texts] }\n",
    "\n",
    "dataset_fmt = dataset_fmt.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.487604Z",
     "iopub.status.busy": "2026-02-03T13:36:12.487110Z",
     "iopub.status.idle": "2026-02-03T13:36:12.492449Z",
     "shell.execute_reply": "2026-02-03T13:36:12.491704Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.487558Z"
    },
    "id": "FqRyirbja1Da",
    "outputId": "50cb982a-4cf8-4ca1-bcb3-0332a4858ed7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWhat are the symptoms of Allergy?<|im_end|>\\n<|im_start|>assistant\\nAllergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.<|im_end|>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fmt[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSM7v1jKxdUD"
   },
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.493830Z",
     "iopub.status.busy": "2026-02-03T13:36:12.493514Z",
     "iopub.status.idle": "2026-02-03T13:36:12.510304Z",
     "shell.execute_reply": "2026-02-03T13:36:12.509594Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.493801Z"
    },
    "id": "6qaylHUKb9HU",
    "outputId": "23ec5510-1c2d-4136-d046-1a9bdb4d1664",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.563 GB.\n",
      "2.27 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:12.511370Z",
     "iopub.status.busy": "2026-02-03T13:36:12.511130Z",
     "iopub.status.idle": "2026-02-03T13:36:15.196096Z",
     "shell.execute_reply": "2026-02-03T13:36:15.195453Z",
     "shell.execute_reply.started": "2026-02-03T13:36:12.511345Z"
    },
    "id": "5zxkTqkX3uHA",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31781e2963be4f4dba92fe4251953071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/5942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_fmt,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 200,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # \"ademamix\"\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "execution": {
     "iopub.execute_input": "2026-02-03T13:36:15.197344Z",
     "iopub.status.busy": "2026-02-03T13:36:15.197105Z",
     "iopub.status.idle": "2026-02-03T13:42:25.800423Z",
     "shell.execute_reply": "2026-02-03T13:42:25.799621Z",
     "shell.execute_reply.started": "2026-02-03T13:36:15.197314Z"
    },
    "id": "E5dqY6CrxfCb",
    "outputId": "8fe097eb-c256-40fa-ea09-6c712b617a36",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,942 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,108,352 of 1,181,448,960 (0.94% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 05:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.778900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.881100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.670200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.885700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.549800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Training completed! Final loss: 1.8382\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"\\nâœ“ Training completed! Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:42:25.801952Z",
     "iopub.status.busy": "2026-02-03T13:42:25.801606Z",
     "iopub.status.idle": "2026-02-03T13:42:25.807697Z",
     "shell.execute_reply": "2026-02-03T13:42:25.807128Z",
     "shell.execute_reply.started": "2026-02-03T13:42:25.801924Z"
    },
    "id": "gZNXrGSebib-",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367.2401 seconds used for training.\n",
      "6.12 minutes used for training.\n",
      "Peak reserved memory = 3.152 GB.\n",
      "Peak reserved memory for training = 0.882 GB.\n",
      "Peak reserved memory % of max memory = 21.644 %.\n",
      "Peak reserved memory for training % of max memory = 6.056 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdHDUavmx1W7"
   },
   "source": [
    "## 7. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:42:25.809040Z",
     "iopub.status.busy": "2026-02-03T13:42:25.808730Z",
     "iopub.status.idle": "2026-02-03T13:42:28.463736Z",
     "shell.execute_reply": "2026-02-03T13:42:28.463177Z",
     "shell.execute_reply.started": "2026-02-03T13:42:25.809006Z"
    },
    "id": "p0mqctMT1cks",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptoms of diabetes include:\n",
      "Frequent urination Frequent thirst Increased hunger Increased fatigue Weight loss<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgeable medical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the common symptoms of diabetes?\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "# 1. Define the \"Stop Tokens\"\n",
    "# We include the standard EOS token and the specific ChatML token <|im_end|>\n",
    "\n",
    "tokenizer.add_special_tokens({\"eos_token\": \"<|im_end|>\"})\n",
    "\n",
    "# 2. Run Generation\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 256,\n",
    "    temperature = 0.1,\n",
    "    top_k = 50,\n",
    "    top_p = 0.1,\n",
    "    repetition_penalty = 1.05,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGsn1kqrxxAH"
   },
   "source": [
    "## 8. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:42:34.828521Z",
     "iopub.status.busy": "2026-02-03T13:42:34.828052Z",
     "iopub.status.idle": "2026-02-03T13:49:54.326989Z",
     "shell.execute_reply": "2026-02-03T13:49:54.326209Z",
     "shell.execute_reply.started": "2026-02-03T13:42:34.828492Z"
    },
    "id": "Cpq4_EKlxztR",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LoRA adapters saved!\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `lfm25_medical_merged`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `lfm25_medical_merged`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9554.22it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/kaggle/working/lfm25_medical_merged`\n",
      "âœ“ Merged model saved!\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `lfm25_medical_gguf`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `lfm25_medical_gguf`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9845.78it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/kaggle/working/lfm25_medical_gguf`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into f16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['LFM2.5-1.2B-Instruct.F16.gguf']\n",
      "Unsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['LFM2.5-1.2B-Instruct.Q4_K_M.gguf']\n",
      "Unsloth: No Ollama template mapping found for model 'unsloth/LFM2.5-1.2B-Instruct'. Skipping Ollama Modelfile\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model LFM2.5-1.2B-Instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "âœ“ GGUF model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters (small size)\n",
    "model.save_pretrained(\"lfm25_medical_lora\")\n",
    "tokenizer.save_pretrained(\"lfm25_medical_lora\")\n",
    "print(\"âœ“ LoRA adapters saved!\")\n",
    "\n",
    "# Merge and save 16-bit model\n",
    "model.save_pretrained_merged(\"lfm25_medical_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "print(\"âœ“ Merged model saved!\")\n",
    "\n",
    "# Export to GGUF for llama.cpp\n",
    "model.save_pretrained_gguf(\"lfm25_medical_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "print(\"âœ“ GGUF model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjEp8AjryC-b"
   },
   "source": [
    "## 9. Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:49:58.753788Z",
     "iopub.status.busy": "2026-02-03T13:49:58.753171Z",
     "iopub.status.idle": "2026-02-03T13:49:58.768328Z",
     "shell.execute_reply": "2026-02-03T13:49:58.767438Z",
     "shell.execute_reply.started": "2026-02-03T13:49:58.753754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae42c16d5354a6196d29e2965234482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment and run to upload to Hugging Face Hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T13:49:54.328309Z",
     "iopub.status.busy": "2026-02-03T13:49:54.328046Z",
     "iopub.status.idle": "2026-02-03T13:49:54.331729Z",
     "shell.execute_reply": "2026-02-03T13:49:54.331125Z",
     "shell.execute_reply.started": "2026-02-03T13:49:54.328279Z"
    },
    "id": "A1y9wzkzyGJ3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"your-username/lfm25-medical-1.2b\", token=True)\n",
    "# tokenizer.push_to_hub(\"your-username/lfm25-medical-1.2b\", token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTcZ0XJJx_Ji"
   },
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned LFM2.5 on medical data!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Train for more steps (increase `max_steps` or use `num_train_epochs`)\n",
    "2. Experiment with different hyperparameters\n",
    "3. Try different medical datasets\n",
    "4. Deploy your model using vLLM or llama.cpp\n",
    "5. Share your model on Hugging Face Hub\n",
    "\n",
    "**Important Reminder:**\n",
    "This model is for educational purposes only. Always consult healthcare professionals for medical advice.\n",
    "\n",
    "**Resources:**\n",
    "- [LFM2.5 Documentation](https://docs.liquid.ai/lfm)\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [GitHub Repository](https://github.com/yourusername/lfm25-medical-finetuning)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
