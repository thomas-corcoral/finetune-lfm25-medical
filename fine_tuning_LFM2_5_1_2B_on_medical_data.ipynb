{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"24bc3c5fba064192b051e44ec7adab54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb63265f2443411db8ed3c54691438fe","IPY_MODEL_558a95946b0d4eabbff017c9e1dfa1af","IPY_MODEL_1186ce5453f540269cac7850205ab8a6"],"layout":"IPY_MODEL_95eeff9b4e6b4a62bdb8324954295b84"}},"eb63265f2443411db8ed3c54691438fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02cad245e24b46559639a410590b880c","placeholder":"â€‹","style":"IPY_MODEL_e8bcdbbcbd10433482a2d2babe18f740","value":"Formattingâ€‡dataset:â€‡100%"}},"558a95946b0d4eabbff017c9e1dfa1af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cfca4d2bc8149a99c7b08950fb7569d","max":5942,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f814bc273c9541698fb7df084536cbdf","value":5942}},"1186ce5453f540269cac7850205ab8a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c615fdd5ac74daf9a64aac7e67a6c32","placeholder":"â€‹","style":"IPY_MODEL_243cba3754de4c60992d2d2f168db6e2","value":"â€‡5942/5942â€‡[00:00&lt;00:00,â€‡7157.42â€‡examples/s]"}},"95eeff9b4e6b4a62bdb8324954295b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02cad245e24b46559639a410590b880c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8bcdbbcbd10433482a2d2babe18f740":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cfca4d2bc8149a99c7b08950fb7569d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f814bc273c9541698fb7df084536cbdf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c615fdd5ac74daf9a64aac7e67a6c32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"243cba3754de4c60992d2d2f168db6e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"169df539dfe545be9bf91691250c44e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f274c5d04fa24f18aada8c422efd3ce1","IPY_MODEL_b725c2474b604b0ba52546da7e110c20","IPY_MODEL_ca27b03757474d95b7a1d56bc0b8e48c"],"layout":"IPY_MODEL_08675a3ec3434f1c9e181e36eae11797"}},"f274c5d04fa24f18aada8c422efd3ce1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_343e411b7c7049c4908ff79f782db068","placeholder":"â€‹","style":"IPY_MODEL_e56d53606d974072a5eea27a656207d4","value":"Map:â€‡100%"}},"b725c2474b604b0ba52546da7e110c20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab40c94ed9dc40ef82741ef3fdc2a5fa","max":5942,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efa4d6afc73c4376ae0fbce7014affdd","value":5942}},"ca27b03757474d95b7a1d56bc0b8e48c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a46589983bf412cb0266b088bb2105f","placeholder":"â€‹","style":"IPY_MODEL_89a7024e75a4478a8f0690b58a8c80b4","value":"â€‡5942/5942â€‡[00:00&lt;00:00,â€‡7134.45â€‡examples/s]"}},"08675a3ec3434f1c9e181e36eae11797":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"343e411b7c7049c4908ff79f782db068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e56d53606d974072a5eea27a656207d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab40c94ed9dc40ef82741ef3fdc2a5fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efa4d6afc73c4376ae0fbce7014affdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a46589983bf412cb0266b088bb2105f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89a7024e75a4478a8f0690b58a8c80b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning LFM2.5-1.2B on Medical Data\n\nThis notebook demonstrates how to fine-tune Liquid AI's LFM2.5-1.2B-Instruct model (Architecture: 16 layers (10 double-gated LIV convolution blocks + 6 GQA blocks)) on medical instruction data using Unsloth.\n\n**Requirements:**\n- GPU: T4 (free on Google Colab)\n- RAM: 12GB+\n- Time: ~15-20 minutes for 100 steps\n\n**Important:**\n1. Runtime â†’ Change runtime type â†’ T4 GPU\n2. Run cells in order\n3. This model is for educational purposes only - not medical advice!","metadata":{"id":"Tec7FtHFuaWT"}},{"cell_type":"markdown","source":"## 1. Install Dependencies","metadata":{"id":"iRJtfEykvhLz"}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.57.3\n!pip install --no-deps trl==0.22.2","metadata":{"id":"FFmo3nM40_Uw","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:33:30.728879Z","iopub.execute_input":"2026-02-03T13:33:30.729394Z","iopub.status.idle":"2026-02-03T13:34:15.713665Z","shell.execute_reply.started":"2026-02-03T13:33:30.729366Z","shell.execute_reply":"2026-02-03T13:34:15.712611Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install --upgrade --no-deps --force-reinstall unsloth unsloth_zoo","metadata":{"id":"VqFtlbRyYid-","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:34:15.715641Z","iopub.execute_input":"2026-02-03T13:34:15.716139Z","iopub.status.idle":"2026-02-03T13:34:15.719933Z","shell.execute_reply.started":"2026-02-03T13:34:15.716110Z","shell.execute_reply":"2026-02-03T13:34:15.719213Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 2. Import Libraries","metadata":{"id":"V3YieocAxGVz"}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# Check GPU\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XT7XdbZsvlav","outputId":"70c4fdc5-336d-45b4-eb1e-515e087a53a5","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:34:15.720882Z","iopub.execute_input":"2026-02-03T13:34:15.721203Z","iopub.status.idle":"2026-02-03T13:35:21.184230Z","shell.execute_reply.started":"2026-02-03T13:34:15.721142Z","shell.execute_reply":"2026-02-03T13:35:21.183365Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-02-03 13:34:28.182320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770125668.583232      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770125668.694449      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770125669.732554      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770125669.732603      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770125669.732606      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770125669.732608      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nGPU Available: True\nGPU Name: Tesla T4\nGPU Memory: 14.56 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 3. Load Model and Tokenizer","metadata":{"id":"xViExoWJxKpO"}},{"cell_type":"code","source":"# Configuration\nmodel_name = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n\n# Load model with 16-bit quantization\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/LFM2.5-1.2B-Instruct\",\n    max_seq_length = 2048, # Can go up to 32,768 for LFM2.5\n    load_in_4bit = False, # 4 bit quantization to reduce memory\n    load_in_8bit = False, # A bit more accurate, uses 2x memory\n    load_in_16bit = True, # Enables 16bit LoRA\n    full_finetuning = False, # We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n    # device_map = \"balanced\",\n)\n\nprint(\"Model loaded successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kmkk-cFBxNeM","outputId":"e8cd62b7-bdd9-48ba-90b3-5d6ea138c7d7","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:35:21.185922Z","iopub.execute_input":"2026-02-03T13:35:21.186623Z","iopub.status.idle":"2026-02-03T13:35:43.347641Z","shell.execute_reply.started":"2026-02-03T13:35:21.186591Z","shell.execute_reply":"2026-02-03T13:35:43.346831Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.1.4: Fast Lfm2 patching. Transformers: 4.57.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796b060ada484d09be81c0a14b158d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f63b7fa7d245c8883e9e5cbed3ff92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aaf48a2a7924d9bae347fc6d8008091"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0815745ae1994ad590b567f16b6f1132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088c17407263434489f59cc7c77f81ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499cd35a20534c0898998a96e777c1db"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a knowledgeable medical assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the common symptoms of diabetes?\"}\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n    tokenize = True,\n    return_dict = True,\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **inputs,\n    max_new_tokens = 128, # Increase for longer outputs!\n    # Recommended Liquid settings!\n    temperature = 0.1, top_k = 50, top_p = 0.1, repetition_penalty = 1.05,\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8fYa2YtZ4d2","outputId":"cbe6d661-5407-400f-beb8-a4397a89b2be","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:35:43.348819Z","iopub.execute_input":"2026-02-03T13:35:43.349228Z","iopub.status.idle":"2026-02-03T13:36:04.473281Z","shell.execute_reply.started":"2026-02-03T13:35:43.349144Z","shell.execute_reply":"2026-02-03T13:36:04.472589Z"}},"outputs":[{"name":"stdout","text":"Diabetes is a chronic condition characterized by elevated blood sugar levels due to problems with insulin production or use. Common symptoms include:\n\n1. **Increased Thirst (Polydipsia)** â€“ The body loses more fluids than usual, leading to frequent urination.\n2. **Frequent Urination (Polyuria)** â€“ Excess glucose in the blood draws water into the urine, increasing urine output.\n3. **Unexplained Weight Loss** â€“ Despite eating more, some people lose weight due to the body breaking down fat and muscle for energy.\n4. **Extreme Fatigue** â€“ High blood sugar can\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 4. Configure LoRA","metadata":{"id":"JQFFRl1IxSGH"}},{"cell_type":"code","source":"# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"in_proj\",\n                      \"w1\", \"w2\", \"w3\"],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\n# Print trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBIY5mb0xUmN","outputId":"1448a1b5-5be2-409f-d58d-0bb7249a324d","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:04.474556Z","iopub.execute_input":"2026-02-03T13:36:04.474837Z","iopub.status.idle":"2026-02-03T13:36:10.047815Z","shell.execute_reply.started":"2026-02-03T13:36:04.474800Z","shell.execute_reply":"2026-02-03T13:36:10.046978Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model` require gradients\nTrainable params: 11,108,352 (0.94%)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 5. Load and Prepare Dataset","metadata":{"id":"-I8QrpuBxXLK"}},{"cell_type":"code","source":"# Load medical dataset\ndataset = load_dataset(\"medalpaca/medical_meadow_wikidoc_patient_information\", split=\"train\")\nprint(f\"Dataset size: {len(dataset)} samples\")\n\n# Show sample\nprint(\"\\nSample data:\")\nprint(dataset[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"No26hEMIxZ3z","outputId":"880fefee-2a1d-4b80-8107-d6d4f32da4a1","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:10.048934Z","iopub.execute_input":"2026-02-03T13:36:10.049573Z","iopub.status.idle":"2026-02-03T13:36:11.707461Z","shell.execute_reply.started":"2026-02-03T13:36:10.049542Z","shell.execute_reply":"2026-02-03T13:36:11.706783Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2067cb4c2b5e496eb1cdc9e96f8eb466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_meadow_wikidoc_patient_info.json:   0%|          | 0.00/3.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09c6a4358484c469a0e2da9ad68a614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5942 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01cbe67160ac464c8aa258f2cd4fe9bb"}},"metadata":{}},{"name":"stdout","text":"Dataset size: 5942 samples\n\nSample data:\n{'input': 'What are the symptoms of Allergy?', 'output': 'Allergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.', 'instruction': 'Answer this question truthfully'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Format dataset for LFM2.5\ndef format_instruction(example):\n    # system_message = \"You are a knowledgeable medical assistant providing accurate health information. Always recommend consulting healthcare professionals for medical advice.\"\n\n    conversation = [\n        # {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": example.get('input', '')},\n        {\"role\": \"assistant\", \"content\": example.get('output', '')}\n    ]\n\n    return {\"text\": conversation}\n\n# Apply formatting\ndataset = dataset.map(\n    format_instruction,\n    remove_columns=dataset.column_names,\n    desc=\"Formatting dataset\"\n)\n\nprint(\"âœ“ Dataset formatted!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["24bc3c5fba064192b051e44ec7adab54","eb63265f2443411db8ed3c54691438fe","558a95946b0d4eabbff017c9e1dfa1af","1186ce5453f540269cac7850205ab8a6","95eeff9b4e6b4a62bdb8324954295b84","02cad245e24b46559639a410590b880c","e8bcdbbcbd10433482a2d2babe18f740","5cfca4d2bc8149a99c7b08950fb7569d","f814bc273c9541698fb7df084536cbdf","5c615fdd5ac74daf9a64aac7e67a6c32","243cba3754de4c60992d2d2f168db6e2"]},"id":"_CryDDmlxbrG","outputId":"db45e43e-1c25-4453-a74c-531944233274","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:11.708438Z","iopub.execute_input":"2026-02-03T13:36:11.708777Z","iopub.status.idle":"2026-02-03T13:36:12.022012Z","shell.execute_reply.started":"2026-02-03T13:36:11.708747Z","shell.execute_reply":"2026-02-03T13:36:12.021223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Formatting dataset:   0%|          | 0/5942 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca57a326809944ba86a4011edf9de070"}},"metadata":{}},{"name":"stdout","text":"âœ“ Dataset formatted!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_data_formats\ndataset_fmt = standardize_data_formats(dataset)","metadata":{"id":"e8lT9hVxabQI","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.023312Z","iopub.execute_input":"2026-02-03T13:36:12.023577Z","iopub.status.idle":"2026-02-03T13:36:12.028939Z","shell.execute_reply.started":"2026-02-03T13:36:12.023551Z","shell.execute_reply":"2026-02-03T13:36:12.028409Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset_fmt[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwKzCwX8ahQB","outputId":"d262ca1f-cce7-463f-c960-eb1b4eb36976","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.031911Z","iopub.execute_input":"2026-02-03T13:36:12.032231Z","iopub.status.idle":"2026-02-03T13:36:12.051141Z","shell.execute_reply.started":"2026-02-03T13:36:12.032202Z","shell.execute_reply":"2026-02-03T13:36:12.050260Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'text': [{'content': 'What are the symptoms of Allergy?', 'role': 'user'},\n  {'content': 'Allergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    texts = tokenizer.apply_chat_template(\n        examples[\"text\"],\n        tokenize = False,\n        add_generation_prompt = False,\n    )\n    return { \"text\" : [x.removeprefix(tokenizer.bos_token) for x in texts] }\n\ndataset_fmt = dataset_fmt.map(formatting_prompts_func, batched = True)","metadata":{"id":"31XuOeGnat0L","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["169df539dfe545be9bf91691250c44e3","f274c5d04fa24f18aada8c422efd3ce1","b725c2474b604b0ba52546da7e110c20","ca27b03757474d95b7a1d56bc0b8e48c","08675a3ec3434f1c9e181e36eae11797","343e411b7c7049c4908ff79f782db068","e56d53606d974072a5eea27a656207d4","ab40c94ed9dc40ef82741ef3fdc2a5fa","efa4d6afc73c4376ae0fbce7014affdd","2a46589983bf412cb0266b088bb2105f","89a7024e75a4478a8f0690b58a8c80b4"]},"outputId":"377904cd-1ba5-4152-9c23-323604951abd","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.052278Z","iopub.execute_input":"2026-02-03T13:36:12.052606Z","iopub.status.idle":"2026-02-03T13:36:12.486191Z","shell.execute_reply.started":"2026-02-03T13:36:12.052566Z","shell.execute_reply":"2026-02-03T13:36:12.485441Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5942 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d039217178842369b5267d553577ecd"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset_fmt[0][\"text\"]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"FqRyirbja1Da","outputId":"50cb982a-4cf8-4ca1-bcb3-0332a4858ed7","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.487110Z","iopub.execute_input":"2026-02-03T13:36:12.487604Z","iopub.status.idle":"2026-02-03T13:36:12.492449Z","shell.execute_reply.started":"2026-02-03T13:36:12.487558Z","shell.execute_reply":"2026-02-03T13:36:12.491704Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nWhat are the symptoms of Allergy?<|im_end|>\\n<|im_start|>assistant\\nAllergy symptoms vary, but may include:\\nBreathing problems (coughing, shortness of breath) Burning, tearing, or itchy eyes Conjunctivitis (red, swollen eyes) Coughing Diarrhea Headache Hives Itching of the nose, mouth, throat, skin, or any other area Runny nose Skin rashes Stomach cramps Vomiting Wheezing\\nWhat part of the body is contacted by the allergen plays a role in the symptoms you develop. For example:\\nAllergens that are breathed in often cause a stuffy nose, itchy nose and throat, mucus production, cough, or wheezing. Allergens that touch the eyes may cause itchy, watery, red, swollen eyes. Eating something you are allergic to can cause nausea, vomiting, abdominal pain, cramping, diarrhea, or a severe, life-threatening reaction. Allergens that touch the skin can cause a skin rash, hives, itching, blisters, or even skin peeling. Drug allergies usually involve the whole body and can lead to a variety of symptoms.<|im_end|>\\n'"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 6. Train the Model","metadata":{"id":"wSM7v1jKxdUD"}},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qaylHUKb9HU","outputId":"23ec5510-1c2d-4136-d046-1a9bdb4d1664","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.493514Z","iopub.execute_input":"2026-02-03T13:36:12.493830Z","iopub.status.idle":"2026-02-03T13:36:12.510304Z","shell.execute_reply.started":"2026-02-03T13:36:12.493801Z","shell.execute_reply":"2026-02-03T13:36:12.509594Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.563 GB.\n2.27 GB of memory reserved.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_fmt,\n    eval_dataset = None, # Can set up evaluation!\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        max_steps = 200,\n        num_train_epochs = 1,\n        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n        logging_steps = 1,\n        optim = \"adamw_8bit\", # \"ademamix\"\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"id":"5zxkTqkX3uHA","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:12.511130Z","iopub.execute_input":"2026-02-03T13:36:12.511370Z","iopub.status.idle":"2026-02-03T13:36:15.196096Z","shell.execute_reply.started":"2026-02-03T13:36:12.511345Z","shell.execute_reply":"2026-02-03T13:36:15.195453Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/5942 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31781e2963be4f4dba92fe4251953071"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Start training\ntrainer_stats = trainer.train()\nprint(f\"\\nâœ“ Training completed! Final loss: {trainer_stats.training_loss:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"E5dqY6CrxfCb","outputId":"8fe097eb-c256-40fa-ea09-6c712b617a36","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:36:15.197105Z","iopub.execute_input":"2026-02-03T13:36:15.197344Z","iopub.status.idle":"2026-02-03T13:42:25.800423Z","shell.execute_reply.started":"2026-02-03T13:36:15.197314Z","shell.execute_reply":"2026-02-03T13:42:25.799621Z"}},"outputs":[{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 5,942 | Num Epochs = 1 | Total steps = 200\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 11,108,352 of 1,181,448,960 (0.94% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:47, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.943900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.099100</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4.074800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.057800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.962100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.891300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.859900</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.289000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.700100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.422200</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.752300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.561900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.667800</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.164400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.406800</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.334100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.100700</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.269500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.025300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.319600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.036500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.055300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.251400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.030400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.129400</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.105200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.146000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.887000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.063500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.889100</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.844700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.125200</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.879800</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.778900</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.775900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.831500</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.941400</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.542500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.973500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.615600</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.607200</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.941800</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.943400</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.746800</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.609500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.301900</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.881100</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.826100</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.660700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.982000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.989500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.906900</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.756100</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.838100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.397000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.841600</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.740500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.059200</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.680400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.685200</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.671500</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.681200</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.784400</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.996600</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.757000</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>1.765200</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.671800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.370000</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.572900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.738900</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>1.748700</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.605500</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.938600</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.433900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.665900</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>1.936200</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>1.695900</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.790000</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.498600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.734000</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.500400</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>1.708100</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.679400</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>1.789900</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.718100</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>1.717900</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>1.872800</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>1.749500</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>1.527000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.792700</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.433500</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.887400</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.658500</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.564800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.734400</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.581400</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>1.863200</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>1.849200</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.583600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.590400</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.803200</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>1.687600</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>1.644200</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>1.860400</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.672000</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.532600</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>1.635400</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>1.833100</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>1.704400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.778400</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>1.800600</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>1.670200</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>1.548900</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>1.627200</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.680200</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>1.763900</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>1.752100</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>1.561500</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>1.642000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.521800</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>1.706300</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>1.954200</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>1.753300</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.677800</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.672600</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>1.674000</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>1.977300</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>1.731300</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>1.512400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.849800</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>1.631100</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>1.852100</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>1.653300</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>1.408200</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.762600</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>1.708000</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>1.745500</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>1.449900</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>1.865700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.571200</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>1.372900</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>1.796600</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>1.875200</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>1.804300</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.505400</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>1.639500</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>1.711000</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>1.815100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>1.556200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.672100</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>1.502500</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>1.579100</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>1.686100</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>1.490300</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.885700</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>1.624600</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>1.636400</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>1.772500</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>1.729300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.739800</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>1.565700</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>2.415000</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>1.823000</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>1.762800</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.979600</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>1.585100</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>1.734000</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>1.645100</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>1.778100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.861700</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>1.608400</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>1.792100</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>1.651000</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>1.524200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.650600</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>1.911800</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>1.730800</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>1.621700</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>1.579700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.663600</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>1.754800</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>1.820700</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>1.675900</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>1.889200</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.783300</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>1.670300</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>1.737600</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>1.831500</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>1.742600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.001100</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>1.511600</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>1.625300</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>1.529400</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>1.533600</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.708100</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>1.669900</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>1.876900</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>1.647600</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>1.861900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.549800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nâœ“ Training completed! Final loss: 1.8382\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"id":"gZNXrGSebib-","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:42:25.801606Z","iopub.execute_input":"2026-02-03T13:42:25.801952Z","iopub.status.idle":"2026-02-03T13:42:25.807697Z","shell.execute_reply.started":"2026-02-03T13:42:25.801924Z","shell.execute_reply":"2026-02-03T13:42:25.807128Z"}},"outputs":[{"name":"stdout","text":"367.2401 seconds used for training.\n6.12 minutes used for training.\nPeak reserved memory = 3.152 GB.\nPeak reserved memory for training = 0.882 GB.\nPeak reserved memory % of max memory = 21.644 %.\nPeak reserved memory for training % of max memory = 6.056 %.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 7. Test the Fine-tuned Model","metadata":{"id":"bdHDUavmx1W7"}},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a knowledgeable medical assistant.\"},\n    {\"role\": \"user\", \"content\": \"What are the common symptoms of diabetes?\"}\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n    tokenize = True,\n    return_dict = True,\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\n# 1. Define the \"Stop Tokens\"\n# We include the standard EOS token and the specific ChatML token <|im_end|>\n\ntokenizer.add_special_tokens({\"eos_token\": \"<|im_end|>\"})\n\n# 2. Run Generation\n_ = model.generate(\n    **inputs,\n    max_new_tokens = 256,\n    temperature = 0.1,\n    top_k = 50,\n    top_p = 0.1,\n    repetition_penalty = 1.05,\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)","metadata":{"id":"p0mqctMT1cks","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:42:25.808730Z","iopub.execute_input":"2026-02-03T13:42:25.809040Z","iopub.status.idle":"2026-02-03T13:42:28.463736Z","shell.execute_reply.started":"2026-02-03T13:42:25.809006Z","shell.execute_reply":"2026-02-03T13:42:28.463177Z"}},"outputs":[{"name":"stdout","text":"Symptoms of diabetes include:\nFrequent urination Frequent thirst Increased hunger Increased fatigue Weight loss<|im_end|>\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 8. Save the Fine-tuned Model","metadata":{"id":"qGsn1kqrxxAH"}},{"cell_type":"code","source":"# Save LoRA adapters (small size)\nmodel.save_pretrained(\"lfm25_medical_lora\")\ntokenizer.save_pretrained(\"lfm25_medical_lora\")\nprint(\"âœ“ LoRA adapters saved!\")\n\n# Merge and save 16-bit model\nmodel.save_pretrained_merged(\"lfm25_medical_merged\", tokenizer, save_method=\"merged_16bit\")\nprint(\"âœ“ Merged model saved!\")\n\n# Export to GGUF for llama.cpp\nmodel.save_pretrained_gguf(\"lfm25_medical_gguf\", tokenizer, quantization_method=\"q4_k_m\")\nprint(\"âœ“ GGUF model saved!\")","metadata":{"id":"Cpq4_EKlxztR","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:42:34.828052Z","iopub.execute_input":"2026-02-03T13:42:34.828521Z","iopub.status.idle":"2026-02-03T13:49:54.326989Z","shell.execute_reply.started":"2026-02-03T13:42:34.828492Z","shell.execute_reply":"2026-02-03T13:49:54.326209Z"}},"outputs":[{"name":"stdout","text":"âœ“ LoRA adapters saved!\nFound HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Copying 1 files from cache to `lfm25_medical_merged`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"Successfully copied all 1 files from cache to `lfm25_medical_merged`\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9554.22it/s]\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/lfm25_medical_merged`\nâœ“ Merged model saved!\nUnsloth: Merging model weights to 16-bit format...\nFound HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Copying 1 files from cache to `lfm25_medical_gguf`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"Successfully copied all 1 files from cache to `lfm25_medical_gguf`\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9845.78it/s]\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/lfm25_medical_gguf`\nUnsloth: Converting to GGUF format...\n==((====))==  Unsloth: Conversion from HF to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: Updating system package directories\nUnsloth: All required system packages already installed!\nUnsloth: Install llama.cpp and building - please wait 1 to 3 minutes\nUnsloth: Cloning llama.cpp repository\nUnsloth: Install GGUF and other packages\nUnsloth: Successfully installed llama.cpp!\nUnsloth: Preparing converter script...\nUnsloth: [1] Converting model into f16 GGUF format.\nThis might take 3 minutes...\nUnsloth: Initial conversion completed! Files: ['LFM2.5-1.2B-Instruct.F16.gguf']\nUnsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\nUnsloth: Model files cleanup...\nUnsloth: All GGUF conversions completed successfully!\nGenerated files: ['LFM2.5-1.2B-Instruct.Q4_K_M.gguf']\nUnsloth: No Ollama template mapping found for model 'unsloth/LFM2.5-1.2B-Instruct'. Skipping Ollama Modelfile\nUnsloth: example usage for text only LLMs: llama-cli --model LFM2.5-1.2B-Instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\nâœ“ GGUF model saved!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## 9. Upload to Hugging Face (Optional)","metadata":{"id":"IjEp8AjryC-b"}},{"cell_type":"code","source":"# Uncomment and run to upload to Hugging Face Hub\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:49:58.753171Z","iopub.execute_input":"2026-02-03T13:49:58.753788Z","iopub.status.idle":"2026-02-03T13:49:58.768328Z","shell.execute_reply.started":"2026-02-03T13:49:58.753754Z","shell.execute_reply":"2026-02-03T13:49:58.767438Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae42c16d5354a6196d29e2965234482"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# model.push_to_hub(\"your-username/lfm25-medical-1.2b\", token=True)\n# tokenizer.push_to_hub(\"your-username/lfm25-medical-1.2b\", token=True)","metadata":{"id":"A1y9wzkzyGJ3","trusted":true,"execution":{"iopub.status.busy":"2026-02-03T13:49:54.328046Z","iopub.execute_input":"2026-02-03T13:49:54.328309Z","iopub.status.idle":"2026-02-03T13:49:54.331729Z","shell.execute_reply.started":"2026-02-03T13:49:54.328279Z","shell.execute_reply":"2026-02-03T13:49:54.331125Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## ðŸŽ‰ Congratulations!\n\nYou've successfully fine-tuned LFM2.5 on medical data!\n\n**Next Steps:**\n1. Train for more steps (increase `max_steps` or use `num_train_epochs`)\n2. Experiment with different hyperparameters\n3. Try different medical datasets\n4. Deploy your model using vLLM or llama.cpp\n5. Share your model on Hugging Face Hub\n\n**Important Reminder:**\nThis model is for educational purposes only. Always consult healthcare professionals for medical advice.\n\n**Resources:**\n- [LFM2.5 Documentation](https://docs.liquid.ai/lfm)\n- [Unsloth Documentation](https://docs.unsloth.ai/)\n- [GitHub Repository](https://github.com/yourusername/lfm25-medical-finetuning)","metadata":{"id":"wTcZ0XJJx_Ji"}}]}